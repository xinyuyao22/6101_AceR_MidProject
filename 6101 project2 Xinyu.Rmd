---
title: "6101 Project2"
author: "Xiaotian Huang"
date: "11/23/2019"
output: html_document
---

```{r basicfcn}
# can add quietly=T option to the require() function
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

## Chapter 1: Introduction of Cardiovascular Disease:

Based on The World Health Organization (WHO), Cardiovascular diseases (CVDs) are disorders related to the heart and blood vessels. The diseases mainly caused by fatty deposits plaque builds up on the inner walls of the blood vessels which prevent prevents blood from flowing to the heart or brain. 

![As plaque builds up in the arteries of a person with heart disease, the inside of the arteries begins to narrow, which lessens or blocks the flow of blood.](CVD picture.jpg)


According to 2016 report, cardiovascular disease remains the leading cause of death in the United States (Benjamin et al., 2019). Around 80% of CVD deaths are a heart attack and stroke. The cause of cardiovascular diseases is usually the presence of a combination of risk factors, such as unhealthy diet, obesity, physical inactivity, tobacco use and harmful use of alcohol.

Since there are many reports indicated that the cause of cardiovascular diseases is associated with our lifestyle. Therefore, we want to use this dataset to validate a person's behavior and developing of the disease.

## Chapter 2: Description of Data and Exploratory Data Analysis

### 2.1 Source Data
The source data for our EDA is a CSV containing 70 000 records of patients data in 12 features: age, height, weight, gender, systolic blood pressure, diastolic blood pressure, cholesterol, glucose, smoking, alcohol intake, physical activity, and presence or absence of cardiovascular disease. (https://www.kaggle.com/sulianova/cardiovascular-disease-dataset)
```{r, include=FALSE}
cardio <- read.csv("cardio_train.csv", sep = ";")
cardio$gender <- as.factor(cardio$gender)
cardio$smoke <- as.factor(cardio$smoke)
cardio$alco <- as.factor(cardio$alco)
cardio$cholesterol <- as.factor(cardio$cholesterol)
cardio$gluc <- as.factor(cardio$gluc)
cardio$active <- as.factor(cardio$active)
cardio$cardio <- as.factor(cardio$cardio)
str(cardio)
```
### 2.2 Preprocessing Data

We noticed that variable 'age' is int(day), which were converted into int(years).As height and weight individually do not mean much to patients' health, so we calculated Body Mass Index (BMI), a measure of body fat based on height and weight that applies to adult men and women, and added it as a feature. Also column 'id' was droped.
```{r, include=F}
cardio <- subset(cardio, select=-c(id))
cardio$age <- round((cardio$age)/365)
cardio$bmi <- cardio$weight/((cardio$height/100)^2)
summary(cardio)
```

We noticed that the min value of systolic blood pressure(ap_hi) and diastolic blood pressure (ap_lo) are negative values, which do not make sense. In addition, diastolic blood pressure is supposed to be lower than systolic blood pressure. The data were further cleaned based on these crterion.

```{r, echo=F}
cardio <- cardio[which(cardio$ap_hi > 0), ]
cardio <- cardio[which(cardio$ap_lo > 0), ]
cardio <- cardio[which(cardio$ap_lo < cardio$ap_hi), ]
```

Then the distribution of age, height, weight, ap_hi and ap_lo was checked.

```{r, echo=F}
library('ggplot2')
ggplot(data=cardio, aes(x=age))+
  geom_histogram(fill="orange", col = "black", binwidth = 5)+
  ggtitle("Histogram of Age")+
  xlab("Age") + 
  ylab("Count") + 
  theme(axis.title = element_text(colour = "#7a7a78"))+
  theme(axis.text = element_text(colour = "#7a7a78"))+
  theme(plot.title= element_text(hjust=0.5, size = 14))
ggplot(data=cardio, aes(x=height))+
  geom_histogram(fill="green", col = "black", binwidth = 10)+
  ggtitle("Histogram of Height")+
  xlab("Height") + 
  ylab("Count") + 
  theme(axis.title = element_text(colour = "#7a7a78"))+
  theme(axis.text = element_text(colour = "#7a7a78"))+
  theme(plot.title= element_text(hjust=0.5, size = 14))
ggplot(data=cardio, aes(x=weight))+
  geom_histogram(fill="yellow", col = "black", binwidth = 10)+
  ggtitle("Histogram of Weight")+
  xlab("Weight") + 
  ylab("Count") + 
  theme(axis.title = element_text(colour = "#7a7a78"))+
  theme(axis.text = element_text(colour = "#7a7a78"))+
  theme(plot.title= element_text(hjust=0.5, size = 14))
ggplot(data=cardio, aes(x=ap_hi))+
  geom_histogram(fill="blue", col = "black", binwidth = 10)+
  ggtitle("Histogram of Systolic Blood Pressure")+
  xlab("Systolic blood pressure") + 
  ylab("Count") + 
  theme(axis.title = element_text(colour = "#7a7a78"))+
  theme(axis.text = element_text(colour = "#7a7a78"))+
  theme(plot.title= element_text(hjust=0.5, size = 14))
ggplot(data=cardio, aes(x=ap_lo))+
  geom_histogram(fill="red", col = "black", binwidth = 10)+
  ggtitle("Histogram of Diastolic Blood Pressure")+
  xlab("Diastolic blood pressure") + 
  ylab("Count") + 
  theme(axis.title = element_text(colour = "#7a7a78"))+
  theme(axis.text = element_text(colour = "#7a7a78"))+
  theme(plot.title= element_text(hjust=0.5, size = 14))
```

The histogram of age shows that there are only few observation for age<35, which could not represent the population of age<35, so the observations with age<35 were droped. For height, weight, ap_hi, and ap_lo, the histograms were way skewed by some extreme outliers, which were droped in this step.

```{r outlierKD_def, include=FALSE}
# modified to allow prompt-free run-through
outlierKD <- function(dt, var, rmv=NULL) { 
     var_name <- eval(substitute(var),eval(dt))
     na1 <- sum(is.na(var_name))
     m1 <- mean(var_name, na.rm = T)
     sd1 <- sd(var_name,na.rm = T)
     par(mfrow=c(2, 2), oma=c(0,0,3,0))
     boxplot(var_name, main="With outliers")
     hist(var_name, main="With outliers", xlab=NA, ylab=NA)
     outlier <- boxplot.stats(var_name)$out
     mo <- mean(outlier)
     var_name <- ifelse(var_name %in% outlier, NA, var_name)
     boxplot(var_name, main="Without outliers")
     hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
     title("Outlier Check", outer=TRUE)
     na2 <- sum(is.na(var_name))
     cat("Outliers identified:", na2 - na1, "n")
     cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "n")
     cat("Mean of the outliers:", round(mo, 2), "n")
     m2 <- mean(var_name, na.rm = T)
     cat("Mean without removing outliers:", round(m1, 2), "n")
     cat("Mean if we remove outliers:", round(m2, 2), "n")
     #
     if(is.null(rmv)) { 
       response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ") 
     } else {
       if (rmv=='y'|rmv=='yes'|rmv=='Y'|rmv=='Yes'|rmv=='YES'|rmv==TRUE ) { response = 'y' } else { response = 'n' }
     }
     #
     if(response == "y" | response == "yes"){
          dt[as.character(substitute(var))] <- invisible(var_name)
          assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
          cat("Outliers successfully removed", "n")
          return(invisible(dt))
     } else{
          cat("Nothing changed", "n")
          return(invisible(var_name))
     }
}
```
```{r, include=FALSE}
cardio <- cardio[which(cardio$age > 35), ]
outlierKD(cardio, height, 'y')
outlierKD(cardio, weight, 'y')
outlierKD(cardio, ap_hi, 'y')
outlierKD(cardio, ap_lo, 'y')
cardio <- na.omit(cardio)
```

## Chapter 3: Cardio
### 3.1 SMART Question

What are the risk factors of cardiovascular diseases? Are all the variables correlated to the development of cardiovascular disease?

### 3.2.1 Basic analyze
```{r logit1, echo=FALSE}
cardiologit1 <- glm(cardio ~ gender + age + ap_hi + ap_lo + cholesterol + bmi + gluc + smoke + alco + active, data = cardio, family = "binomial")
summary(cardiologit1)
loadPkg('car')
vif(cardiologit1)
```
Here we use GVIF to check whether collinearity is a problem in this logistic regression model. Typically, GVIF only comes into play for factors and polynomial variables. Variables which require more than 1 coefficient and thus more than 1 degree of freedom are typically evaluated using the GVIF. For one-coefficient terms VIF equals GVIF. The rule of GVIF2(1/(2Ã—Df))<2 is applied, which would equal a VIF of 4 for one-coefficient variables. Thus, here in our logistic regression model, collinearity is not a problem, and all the coefficients, except gender, are found significant (small p-values). Thus, gender is dropped.

```{r logit, echo=FALSE}
cardiologit <- glm(cardio ~ age + ap_hi + ap_lo + cholesterol + bmi + gluc + smoke + alco + active, data = cardio, family = "binomial")
summary(cardiologit)
```

All the coefficients are found significant (small p-values).

#### 3.2.2 Hosmer and Lemeshow test  

The Hosmer and Lemeshow Goodness of Fit test can be used to evaluate logistic regression fit. 

```{r HosmerLemeshow}
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
cardiologitHoslem = hoslem.test(cardio$cardio, fitted(cardiologit)) # Hosmer and Lemeshow test, a chi-squared test
detach("package:ResourceSelection", unload = T) # good habit to remove unload packages no longer needed 
```

The result is shown here:  
```{r HosmerLemeshowRes, results='markup', collapse=F}
cardiologitHoslem
```

The p-value of `r admitLogitHoslem$p.value` is smaller than 0.05. This indicates the model is a good fit

#### 3.2.3 ROC curve and AUC

```{r roc_auc}
loadPkg("pROC")
prob=predict(cardiologit, type = c("response"))
cardio$prob=prob
h <- roc(cardio~prob, data=cardio)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# detach("package:pROC", unload = T) # good habit to remove unload packages no longer needed 
```

We have here the area-under-curve of `r auc(h)`, which is slightly less than 0.8. This test evaluates the model as a not so good fit. 

#### 3.2.4 McFadden  

```{r McFadden}
loadPkg("pscl")
cardioLogitpr = pR2(cardiologit)
cardioLogitpr
# detach("package:pscl", unload = T) # good habit to remove unload packages no longer needed 
```

With the McFadden value of `r cardioLogitpr['McFadden']`, which is analgous to the coefficient of determination R$2$, about 18.9% of the variations in cardio is explained by the explanatory variables in the model.

According to the three model evaluation, this logistic regression is a relatively ok model.

## Chapter 5: KNN

```{r 5.1, echo=F}
loadPkg("FNN")
cardioKnn <- subset(cardio, select = -c(gender, cholesterol, gluc, smoke, alco, active, height, weight, prob, cardio))
cardioKnn$ap_hi <- as.numeric(cardioKnn$ap_hi)
cardioKnn$ap_lo <- as.numeric(cardioKnn$ap_lo)
#first we want to scale the data so KNN will operate correctly
scaledcardio <- as.data.frame(scale(cardioKnn, center = TRUE, scale = TRUE))
set.seed(1)
cardio_train_rows = sample(1:nrow(scaledcardio),
                           round(0.7 * nrow(scaledcardio), 0),
                           replace = FALSE)
cardio_training <- scaledcardio[cardio_train_rows,]
cardio_test <- scaledcardio[-cardio_train_rows,]
```

```{r 5.2, include=F}
cardio.trainLabels <- cardio[cardio_train_rows, 12]
cardio.testLabels <- cardio[-cardio_train_rows, 12]
```

```{r Q3, echo=F}
#So now we will deploy our model 
cardio_40NN <- knn(train = cardio_training, test = cardio_test, cl=cardio.trainLabels, k=40)
#install.packages("gmodels")
loadPkg("gmodels")
cardioPREDCross <- CrossTable(cardio.testLabels, cardio_40NN, prop.chisq = FALSE)
cardio_250NN <- knn(train = cardio_training, test = cardio_test, cl=cardio.trainLabels, k=250)
cardioPREDCross <- CrossTable(cardio.testLabels, cardio_250NN, prop.chisq = FALSE)
cardio_1575NN <- knn(train = cardio_training, test = cardio_test, cl=cardio.trainLabels, k=1575)
logregPREDCross <- CrossTable(cardio.testLabels, cardio_1575NN, prop.chisq = FALSE)
```

```{r Q41, echo=F}
kNN_res = table(cardio_40NN,
                cardio.testLabels)
kNN_acc40 = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc40

kNN_res = table(cardio_250NN,
                cardio.testLabels)
kNN_acc250 = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc250

kNN_res = table(cardio_1575NN,
                cardio.testLabels)
kNN_acc1575 = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc1575

```
k equal n$1/3$, n$1/2$,and n$2/3$ are selected. The accuracies for k=40 is 71.3%; for k=250 is 71.6%; for k=1575 is 71.6%.

### Selecting the correct "k"
How does "k" affect classification accuracy? Let's create a function to calculate classification accuracy based on the number of "k."
```{r}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(41, 1575, by = 150),
                         function(x) chooseK(x, 
                                             train_set = cardio_training,
                                             val_set = cardio_test,
                                             train_class = cardio.trainLabels,
                                             val_class = cardio.testLabels))

# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)

```
It seems 5-nearest neighbors is a decent choice because that's the greatest improvement in predictive accuracy before the incremental improvement trails off.


# K-means example:
```{r}
v1=c(2,3,4,10,11,12,20,25,30) # use the example in the pptx, try k=2 clusters
for( cut in 2:(length(v1)-2) ) {
    print(
        paste(
            "mean 1 = ",mean(v1[1:cut]), 
            "mean 2 = ", mean(v1[(cut+1):length(v1)]),
            "Sum of Sqaures  = ", (cut-1)*var(v1[1:cut]) + (length(v1)-cut-1)*var(v1[(cut+1):length(v1)]),
            "cluster 1 len = ", cut, "cluster 2 len = ", (length(v1) - cut)
        )
    ) 
}

```
We minimize the total sum of squares within clusters, and we found the split is 6,3. 

We can also simply use the function kmeans in R like this: 
```{r}
v1clusters <- kmeans(v1,2)
v1clusters
```
```{r}
v2 <- as.data.frame(v1)
colnames(v2) <- c("var1")
v2$var2 <- c(30,21,26,15,8,11,5,9,6)
plot(v2)
v2
```

```{r}
v2clusters3 <- kmeans(v2,5)
v2clusters3
```

```{r}
v2clusters2 <- kmeans(v2,2)
v2clusters2
```